<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.0" />
<title>deepposekit.models.StackedDenseNet API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deepposekit.models.StackedDenseNet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# Copyright 2018-2019 Jacob M. Graving &lt;jgraving@gmail.com&gt;
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#    http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import numpy as np
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import BatchNormalization

import deepposekit.utils.image as image_utils
from deepposekit.models.engine import BaseModel
from deepposekit.models.layers.util import ImageNormalization, Float
from deepposekit.models.layers.deeplabcut import ImageNetPreprocess
from deepposekit.models.layers.densenet import (
    FrontEnd,
    ImageNetFrontEnd,
    DenseNet,
    OutputChannels,
    Concatenate,
)


class StackedDenseNet(BaseModel):
    def __init__(
        self,
        train_generator,
        n_stacks=1,
        n_transitions=-1,
        growth_rate=48,
        bottleneck_factor=1,
        compression_factor=0.5,
        pretrained=False,
        subpixel=True,
        **kwargs
    ):
        &#34;&#34;&#34;
        Define a Stacked DenseNet model
        for pose estimation [1].
        See `References` for details on the model architecture.

        Parameters
        ----------
        train_generator : class deepposekit.io.TrainingGenerator
            A deepposekit.io.TrainingGenerator class for generating
            images and confidence maps.
        n_stacks : int, default = 1
            The number of encoder-decoder networks to stack
            with intermediate supervision between stacks
        n_transitions : int, default = -1
            The number of transition layers (downsampling and upsampling)
            in each encoder-decoder stack. If value is &lt;0
            the number of transitions will be automatically set
            based on image size as the maximum number of possible
            transitions minus n_transitions plus 1, or:
            n_transitions = max_transitions - n_transitions + 1.
            The default is -1, which uses the maximum number of
            transitions possible.
        growth_rate : int, default = 48
            The number of channels to output from each convolutional
            block.
        bottleneck_factor : int, default = 1
            The factor for determining the number of input channels
            to 3x3 convolutional layer in each convolutional block.
            Inputs are first passed through a 1x1 convolutional layer to
            reduce the number of channels to:
            growth_rate * bottleneck_factor
        compression_factor : int, default = 0.5
            The factor for determining the number of channels passed
            through a transition layer (downsampling or upsampling).
            Inputs are first passed through a 1x1 convolutional layer
            to reduce the number of channels to
            n_input_channels * compression_factor
        pretrained : bool, default = False
            Whether to use an encoder that is pretrained on ImageNet
        subpixel: bool, default = True
            Whether to use subpixel maxima for calculating
            keypoint coordinates in the prediction model.

        Attributes
        -------
        train_model: keras.Model
            A model for training the network to produce confidence maps with
            one input layer for images and `n_outputs` output layers for training
            with intermediate supervision
        predict_model: keras.Model
            A model for predicting keypoint coordinates with one input and one output
            using with Maxima2D or SubpixelMaxima2D layers at the output of the network.

        Both of these models share the same computational graph, so training train_model
        updates the weights of predict_model

        References
        ----------
        [1] Graving, J.M., Chae, D., Naik, H., Li, L., Koger, B., Costelloe, B.R.,
            Couzin, I.D. (2019) DeepPoseKit, a software toolkit for fast and robust
            animal pose estimation using deep learning. eLife, 8, e47994
        [2] Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., &amp; Bengio, Y. (2017).
            The one hundred layers tiramisu: Fully convolutional densenets for
            semantic segmentation. In Computer Vision and Pattern Recognition
            Workshops (CVPRW), 2017 IEEE Conference on (pp. 1175-1183). IEEE.
        [3] Newell, A., Yang, K., &amp; Deng, J. (2016). Stacked hourglass networks
            for human pose estimation. In European Conference on Computer
            Vision (pp. 483-499). Springer, Cham.
        [4] Huang, G., Liu, Z., Weinberger, K. Q., &amp; van der Maaten, L. (2017).
            Densely connected convolutional networks. In Proceedings of the IEEE
            conference on computer vision and pattern recognition
            (Vol. 1, No. 2, p. 3).
        [5] Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017).
            Self-normalizing neural networks. In Advances in Neural Information
            Processing Systems (pp. 972-981).
        &#34;&#34;&#34;

        self.n_stacks = n_stacks
        self.growth_rate = growth_rate
        self.bottleneck_factor = bottleneck_factor
        self.compression_factor = compression_factor
        self.n_transitions = n_transitions
        self.pretrained = pretrained
        super(StackedDenseNet, self).__init__(train_generator, subpixel, **kwargs)

    def __init_model__(self):
        max_transitions = np.min(
            [
                image_utils.n_downsample(self.train_generator.height),
                image_utils.n_downsample(self.train_generator.width),
            ]
        )

        n_transitions = self.n_transitions
        if isinstance(n_transitions, (int, np.integer)):
            if n_transitions == 0:
                raise ValueError(&#34;n_transitions cannot equal zero&#34;)
            if n_transitions &lt; 0:
                n_transitions += 1
                n_transitions = max_transitions - np.abs(n_transitions)
                self.n_transitions = n_transitions
            elif 0 &lt; n_transitions &lt;= max_transitions:
                self.n_transitions = n_transitions
            else:
                raise ValueError(
                    &#34;n_transitions must be in range {0} &#34;
                    &#34;&lt; n_transitions &lt;= &#34;
                    &#34;{1}&#34;.format(-max_transitions + 1, max_transitions)
                )
        else:
            raise TypeError(
                &#34;n_transitions must be integer in range &#34;
                &#34;{0} &lt; n_transitions &lt;= &#34;
                &#34;{1}&#34;.format(-max_transitions + 1, max_transitions)
            )

        batch_shape = (
            None,
            self.train_generator.height,
            self.train_generator.width,
            self.train_generator.n_channels,
        )

        if self.train_generator.downsample_factor &lt; 2:
            raise ValueError(
                &#34;StackedDenseNet is only compatible with `downsample_factor` &gt;= 2.&#34;
                &#34;Adjust the TrainingGenerator or choose a different model.&#34;
            )
        if n_transitions &lt;= self.train_generator.downsample_factor:
            raise ValueError(
                &#34;`n_transitions` &lt;= `downsample_factor`. Increase `n_transitions` or decrease `downsample_factor`.&#34;
                &#34; If `n_transitions` is -1 (the default), check that your image resolutions can be repeatedly downsampled (are divisible by 2 repeatedly).&#34;
            )
        input_layer = Input(batch_shape=batch_shape, dtype=&#34;uint8&#34;)
        to_float = Float()(input_layer)
        if self.pretrained:
            if batch_shape[-1] is 1:
                to_float = Concatenate()([to_float] * 3)
                batch_shape = batch_shape[:-1] + (3,)
            normalized = ImageNetPreprocess(&#34;densenet121&#34;)(to_float)
            front_outputs = ImageNetFrontEnd(
                input_shape=batch_shape[1:],
                n_downsample=self.train_generator.downsample_factor
            )(normalized)
        else:
            normalized = ImageNormalization()(to_float)
            front_outputs = FrontEnd(
                growth_rate=self.growth_rate,
                n_downsample=self.train_generator.downsample_factor,
                compression_factor=self.compression_factor,
                bottleneck_factor=self.bottleneck_factor,
            )(normalized)
        n_downsample = self.n_transitions - self.train_generator.downsample_factor
        outputs = front_outputs
        model_outputs = OutputChannels(
            self.train_generator.n_output_channels, name=&#34;output_0&#34;
        )(outputs)

        model_outputs_list = [model_outputs]
        outputs.append(BatchNormalization()(model_outputs))
        for idx in range(self.n_stacks):
            outputs = DenseNet(
                growth_rate=self.growth_rate,
                n_downsample=self.n_transitions
                - self.train_generator.downsample_factor,
                downsample_factor=self.train_generator.downsample_factor,
                compression_factor=self.compression_factor,
                bottleneck_factor=self.bottleneck_factor,
            )(outputs)
            outputs.append(Concatenate()(front_outputs))
            outputs.append(BatchNormalization()(model_outputs))
            model_outputs = OutputChannels(
                self.train_generator.n_output_channels, name=&#34;output_&#34; + str(idx + 1)
            )(outputs)
            model_outputs_list.append(model_outputs)

        self.train_model = Model(
            input_layer, model_outputs_list, name=self.__class__.__name__
        )

    def get_config(self):
        config = {
            &#34;name&#34;: self.__class__.__name__,
            &#34;n_stacks&#34;: self.n_stacks,
            &#34;n_transitions&#34;: self.n_transitions,
            &#34;growth_rate&#34;: self.growth_rate,
            &#34;bottleneck_factor&#34;: self.bottleneck_factor,
            &#34;compression_factor&#34;: self.compression_factor,
            &#34;pretrained&#34;: self.pretrained,
            &#34;subpixel&#34;: self.subpixel,
        }
        base_config = super(StackedDenseNet, self).get_config()
        return dict(list(config.items()) + list(base_config.items()))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deepposekit.models.StackedDenseNet.StackedDenseNet"><code class="flex name class">
<span>class <span class="ident">StackedDenseNet</span></span>
<span>(</span><span>train_generator, n_stacks=1, n_transitions=-1, growth_rate=48, bottleneck_factor=1, compression_factor=0.5, pretrained=False, subpixel=True, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Define a Stacked DenseNet model
for pose estimation [1].
See <code>References</code> for details on the model architecture.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_generator</code></strong> :&ensp;<code>class</code> <a title="deepposekit.io.TrainingGenerator" href="../io/TrainingGenerator.html"><code>deepposekit.io.TrainingGenerator</code></a></dt>
<dd>A deepposekit.io.TrainingGenerator class for generating
images and confidence maps.</dd>
<dt><strong><code>n_stacks</code></strong> :&ensp;<code>int</code>, default = <code>1</code></dt>
<dd>The number of encoder-decoder networks to stack
with intermediate supervision between stacks</dd>
<dt><strong><code>n_transitions</code></strong> :&ensp;<code>int</code>, default = -<code>1</code></dt>
<dd>The number of transition layers (downsampling and upsampling)
in each encoder-decoder stack. If value is &lt;0
the number of transitions will be automatically set
based on image size as the maximum number of possible
transitions minus n_transitions plus 1, or:
n_transitions = max_transitions - n_transitions + 1.
The default is -1, which uses the maximum number of
transitions possible.</dd>
<dt><strong><code>growth_rate</code></strong> :&ensp;<code>int</code>, default = <code>48</code></dt>
<dd>The number of channels to output from each convolutional
block.</dd>
<dt><strong><code>bottleneck_factor</code></strong> :&ensp;<code>int</code>, default = <code>1</code></dt>
<dd>The factor for determining the number of input channels
to 3x3 convolutional layer in each convolutional block.
Inputs are first passed through a 1x1 convolutional layer to
reduce the number of channels to:
growth_rate * bottleneck_factor</dd>
<dt><strong><code>compression_factor</code></strong> :&ensp;<code>int</code>, default = <code>0.5</code></dt>
<dd>The factor for determining the number of channels passed
through a transition layer (downsampling or upsampling).
Inputs are first passed through a 1x1 convolutional layer
to reduce the number of channels to
n_input_channels * compression_factor</dd>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code>, default = <code>False</code></dt>
<dd>Whether to use an encoder that is pretrained on ImageNet</dd>
<dt><strong><code>subpixel</code></strong> :&ensp;<code>bool</code>, default = <code>True</code></dt>
<dd>Whether to use subpixel maxima for calculating
keypoint coordinates in the prediction model.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>train_model</code></strong> :&ensp;<code>keras.Model</code></dt>
<dd>A model for training the network to produce confidence maps with
one input layer for images and <code>n_outputs</code> output layers for training
with intermediate supervision</dd>
<dt><strong><code>predict_model</code></strong> :&ensp;<code>keras.Model</code></dt>
<dd>A model for predicting keypoint coordinates with one input and one output
using with Maxima2D or SubpixelMaxima2D layers at the output of the network.</dd>
</dl>
<p>Both of these models share the same computational graph, so training train_model
updates the weights of predict_model</p>
<h2 id="references">References</h2>
<p>[1] Graving, J.M., Chae, D., Naik, H., Li, L., Koger, B., Costelloe, B.R.,
Couzin, I.D. (2019) DeepPoseKit, a software toolkit for fast and robust
animal pose estimation using deep learning. eLife, 8, e47994
[2] Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., &amp; Bengio, Y. (2017).
The one hundred layers tiramisu: Fully convolutional densenets for
semantic segmentation. In Computer Vision and Pattern Recognition
Workshops (CVPRW), 2017 IEEE Conference on (pp. 1175-1183). IEEE.
[3] Newell, A., Yang, K., &amp; Deng, J. (2016). Stacked hourglass networks
for human pose estimation. In European Conference on Computer
Vision (pp. 483-499). Springer, Cham.
[4] Huang, G., Liu, Z., Weinberger, K. Q., &amp; van der Maaten, L. (2017).
Densely connected convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition
(Vol. 1, No. 2, p. 3).
[5] Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017).
Self-normalizing neural networks. In Advances in Neural Information
Processing Systems (pp. 972-981).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StackedDenseNet(BaseModel):
    def __init__(
        self,
        train_generator,
        n_stacks=1,
        n_transitions=-1,
        growth_rate=48,
        bottleneck_factor=1,
        compression_factor=0.5,
        pretrained=False,
        subpixel=True,
        **kwargs
    ):
        &#34;&#34;&#34;
        Define a Stacked DenseNet model
        for pose estimation [1].
        See `References` for details on the model architecture.

        Parameters
        ----------
        train_generator : class deepposekit.io.TrainingGenerator
            A deepposekit.io.TrainingGenerator class for generating
            images and confidence maps.
        n_stacks : int, default = 1
            The number of encoder-decoder networks to stack
            with intermediate supervision between stacks
        n_transitions : int, default = -1
            The number of transition layers (downsampling and upsampling)
            in each encoder-decoder stack. If value is &lt;0
            the number of transitions will be automatically set
            based on image size as the maximum number of possible
            transitions minus n_transitions plus 1, or:
            n_transitions = max_transitions - n_transitions + 1.
            The default is -1, which uses the maximum number of
            transitions possible.
        growth_rate : int, default = 48
            The number of channels to output from each convolutional
            block.
        bottleneck_factor : int, default = 1
            The factor for determining the number of input channels
            to 3x3 convolutional layer in each convolutional block.
            Inputs are first passed through a 1x1 convolutional layer to
            reduce the number of channels to:
            growth_rate * bottleneck_factor
        compression_factor : int, default = 0.5
            The factor for determining the number of channels passed
            through a transition layer (downsampling or upsampling).
            Inputs are first passed through a 1x1 convolutional layer
            to reduce the number of channels to
            n_input_channels * compression_factor
        pretrained : bool, default = False
            Whether to use an encoder that is pretrained on ImageNet
        subpixel: bool, default = True
            Whether to use subpixel maxima for calculating
            keypoint coordinates in the prediction model.

        Attributes
        -------
        train_model: keras.Model
            A model for training the network to produce confidence maps with
            one input layer for images and `n_outputs` output layers for training
            with intermediate supervision
        predict_model: keras.Model
            A model for predicting keypoint coordinates with one input and one output
            using with Maxima2D or SubpixelMaxima2D layers at the output of the network.

        Both of these models share the same computational graph, so training train_model
        updates the weights of predict_model

        References
        ----------
        [1] Graving, J.M., Chae, D., Naik, H., Li, L., Koger, B., Costelloe, B.R.,
            Couzin, I.D. (2019) DeepPoseKit, a software toolkit for fast and robust
            animal pose estimation using deep learning. eLife, 8, e47994
        [2] Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., &amp; Bengio, Y. (2017).
            The one hundred layers tiramisu: Fully convolutional densenets for
            semantic segmentation. In Computer Vision and Pattern Recognition
            Workshops (CVPRW), 2017 IEEE Conference on (pp. 1175-1183). IEEE.
        [3] Newell, A., Yang, K., &amp; Deng, J. (2016). Stacked hourglass networks
            for human pose estimation. In European Conference on Computer
            Vision (pp. 483-499). Springer, Cham.
        [4] Huang, G., Liu, Z., Weinberger, K. Q., &amp; van der Maaten, L. (2017).
            Densely connected convolutional networks. In Proceedings of the IEEE
            conference on computer vision and pattern recognition
            (Vol. 1, No. 2, p. 3).
        [5] Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017).
            Self-normalizing neural networks. In Advances in Neural Information
            Processing Systems (pp. 972-981).
        &#34;&#34;&#34;

        self.n_stacks = n_stacks
        self.growth_rate = growth_rate
        self.bottleneck_factor = bottleneck_factor
        self.compression_factor = compression_factor
        self.n_transitions = n_transitions
        self.pretrained = pretrained
        super(StackedDenseNet, self).__init__(train_generator, subpixel, **kwargs)

    def __init_model__(self):
        max_transitions = np.min(
            [
                image_utils.n_downsample(self.train_generator.height),
                image_utils.n_downsample(self.train_generator.width),
            ]
        )

        n_transitions = self.n_transitions
        if isinstance(n_transitions, (int, np.integer)):
            if n_transitions == 0:
                raise ValueError(&#34;n_transitions cannot equal zero&#34;)
            if n_transitions &lt; 0:
                n_transitions += 1
                n_transitions = max_transitions - np.abs(n_transitions)
                self.n_transitions = n_transitions
            elif 0 &lt; n_transitions &lt;= max_transitions:
                self.n_transitions = n_transitions
            else:
                raise ValueError(
                    &#34;n_transitions must be in range {0} &#34;
                    &#34;&lt; n_transitions &lt;= &#34;
                    &#34;{1}&#34;.format(-max_transitions + 1, max_transitions)
                )
        else:
            raise TypeError(
                &#34;n_transitions must be integer in range &#34;
                &#34;{0} &lt; n_transitions &lt;= &#34;
                &#34;{1}&#34;.format(-max_transitions + 1, max_transitions)
            )

        batch_shape = (
            None,
            self.train_generator.height,
            self.train_generator.width,
            self.train_generator.n_channels,
        )

        if self.train_generator.downsample_factor &lt; 2:
            raise ValueError(
                &#34;StackedDenseNet is only compatible with `downsample_factor` &gt;= 2.&#34;
                &#34;Adjust the TrainingGenerator or choose a different model.&#34;
            )
        if n_transitions &lt;= self.train_generator.downsample_factor:
            raise ValueError(
                &#34;`n_transitions` &lt;= `downsample_factor`. Increase `n_transitions` or decrease `downsample_factor`.&#34;
                &#34; If `n_transitions` is -1 (the default), check that your image resolutions can be repeatedly downsampled (are divisible by 2 repeatedly).&#34;
            )
        input_layer = Input(batch_shape=batch_shape, dtype=&#34;uint8&#34;)
        to_float = Float()(input_layer)
        if self.pretrained:
            if batch_shape[-1] is 1:
                to_float = Concatenate()([to_float] * 3)
                batch_shape = batch_shape[:-1] + (3,)
            normalized = ImageNetPreprocess(&#34;densenet121&#34;)(to_float)
            front_outputs = ImageNetFrontEnd(
                input_shape=batch_shape[1:],
                n_downsample=self.train_generator.downsample_factor
            )(normalized)
        else:
            normalized = ImageNormalization()(to_float)
            front_outputs = FrontEnd(
                growth_rate=self.growth_rate,
                n_downsample=self.train_generator.downsample_factor,
                compression_factor=self.compression_factor,
                bottleneck_factor=self.bottleneck_factor,
            )(normalized)
        n_downsample = self.n_transitions - self.train_generator.downsample_factor
        outputs = front_outputs
        model_outputs = OutputChannels(
            self.train_generator.n_output_channels, name=&#34;output_0&#34;
        )(outputs)

        model_outputs_list = [model_outputs]
        outputs.append(BatchNormalization()(model_outputs))
        for idx in range(self.n_stacks):
            outputs = DenseNet(
                growth_rate=self.growth_rate,
                n_downsample=self.n_transitions
                - self.train_generator.downsample_factor,
                downsample_factor=self.train_generator.downsample_factor,
                compression_factor=self.compression_factor,
                bottleneck_factor=self.bottleneck_factor,
            )(outputs)
            outputs.append(Concatenate()(front_outputs))
            outputs.append(BatchNormalization()(model_outputs))
            model_outputs = OutputChannels(
                self.train_generator.n_output_channels, name=&#34;output_&#34; + str(idx + 1)
            )(outputs)
            model_outputs_list.append(model_outputs)

        self.train_model = Model(
            input_layer, model_outputs_list, name=self.__class__.__name__
        )

    def get_config(self):
        config = {
            &#34;name&#34;: self.__class__.__name__,
            &#34;n_stacks&#34;: self.n_stacks,
            &#34;n_transitions&#34;: self.n_transitions,
            &#34;growth_rate&#34;: self.growth_rate,
            &#34;bottleneck_factor&#34;: self.bottleneck_factor,
            &#34;compression_factor&#34;: self.compression_factor,
            &#34;pretrained&#34;: self.pretrained,
            &#34;subpixel&#34;: self.subpixel,
        }
        base_config = super(StackedDenseNet, self).get_config()
        return dict(list(config.items()) + list(base_config.items()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deepposekit.models.engine.BaseModel" href="engine.html#deepposekit.models.engine.BaseModel">BaseModel</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepposekit.models.StackedDenseNet.StackedDenseNet.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#34;name&#34;: self.__class__.__name__,
        &#34;n_stacks&#34;: self.n_stacks,
        &#34;n_transitions&#34;: self.n_transitions,
        &#34;growth_rate&#34;: self.growth_rate,
        &#34;bottleneck_factor&#34;: self.bottleneck_factor,
        &#34;compression_factor&#34;: self.compression_factor,
        &#34;pretrained&#34;: self.pretrained,
        &#34;subpixel&#34;: self.subpixel,
    }
    base_config = super(StackedDenseNet, self).get_config()
    return dict(list(config.items()) + list(base_config.items()))</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deepposekit.models.engine.BaseModel" href="engine.html#deepposekit.models.engine.BaseModel">BaseModel</a></b></code>:
<ul class="hlist">
<li><code><a title="deepposekit.models.engine.BaseModel.fit" href="engine.html#deepposekit.models.engine.BaseModel.fit">fit</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deepposekit.models" href="index.html">deepposekit.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deepposekit.models.StackedDenseNet.StackedDenseNet" href="#deepposekit.models.StackedDenseNet.StackedDenseNet">StackedDenseNet</a></code></h4>
<ul class="">
<li><code><a title="deepposekit.models.StackedDenseNet.StackedDenseNet.get_config" href="#deepposekit.models.StackedDenseNet.StackedDenseNet.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.0</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>